{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "710c9bc2",
   "metadata": {},
   "source": [
    "# Virtual Staining Tutorial: KI67 Synthesis with Deep Learning\n",
    "\n",
    "This tutorial demonstrates how to build and train a deep learning model to virtually stain hematoxylin and eosin (H&E) images into corresponding immunohistochemistry (IHC) stained images using the **KI67** marker. \n",
    "\n",
    "We will walk through:\n",
    "- Setting up the dataset and preprocessing\n",
    "- Defining the generator and discriminator models\n",
    "- Training with a combination of adversarial, pixel-wise, and perceptual losses\n",
    "- Evaluating and visualizing the results\n",
    "\n",
    "Dataset: *Acrobat 2023* (Virtual Staining Subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07c874d-118d-4a5f-92b2-6d67a4b129e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6066e9-7ab5-46bb-bb8e-d4384867ccf2",
   "metadata": {},
   "source": [
    "### Dataset Setup\n",
    "\n",
    "This section defines the dataset used for virtual staining. We load paired H&E and KI67-stained whole slide images (WSIs) from the ACROBAT 2023 dataset that have been co-registered using VALIS and tiled into 256 x 256 patches at 40X magnification.\n",
    "\n",
    "During training images are transformed using augmentations (e.g., flipping, rotating, normalization) to increase robustness and generalization. These tiles are loaded into a PyTorch Dataset that returns paired source and target images for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163f94d0-1f86-4218-87f1-33de268fa60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/ix1/qgu/ngl18/GLBIO2025_VirtualStaining\"\n",
    "train_source_files = sorted(list(glob.iglob(f\"{root_dir}/data/train/he/**.npy\")))\n",
    "train_target_files = sorted(list(glob.iglob(f\"{root_dir}/data/train/ki67/**.npy\")))\n",
    "val_source_files = sorted(list(glob.iglob(f\"{root_dir}/data/valid/he/**.npy\")))\n",
    "val_target_files = sorted(list(glob.iglob(f\"{root_dir}/data/valid/ki67/**.npy\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d6a9c7-36b1-4c81-b93b-9f1f13d32b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import VirtualStainingDataset\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Affine(scale=(0.98, 1.02), translate_percent=0.05, rotate=(-5, 5), p=0.7),\n",
    "    A.ElasticTransform(alpha=1, sigma=50, alpha_affine=5, p=0.2),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05, p=0.7),\n",
    "    A.RandomGamma(p=0.3),\n",
    "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    ToTensorV2()\n",
    "], additional_targets={'target': 'image'}) \n",
    "\n",
    "val_test_transform = A.Compose([\n",
    "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    ToTensorV2()\n",
    "], additional_targets={'target': 'image'}) \n",
    "\n",
    "train_virtual_staining_dataset = VirtualStainingDataset(train_source_files, train_target_files, \n",
    "                                                        transform=train_transform)\n",
    "train_virtual_staining_dataloader = DataLoader(train_virtual_staining_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_virtual_staining_dataset = VirtualStainingDataset(val_source_files, val_target_files, \n",
    "                                                        transform=val_test_transform)\n",
    "val_virtual_staining_dataloader = DataLoader(val_virtual_staining_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d22a3de",
   "metadata": {},
   "source": [
    "### Model Setup\n",
    "We use the **Pix2Pix** model, a conditional Generative Adversarial Network (cGAN) introduced by Isola et al. (2017) for image-to-image translation tasks. Pix2Pix consists of two main components:\n",
    "\n",
    "- **Generator (G)**: Learns to map input H&E images to the corresponding KI67-stained images. It typically uses a U-Net architecture to preserve spatial features.\n",
    "- **Discriminator (D)**: A PatchGAN discriminator evaluates the realism of local image patches, helping the generator produce realistic, high-frequency details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc821bb6-fe47-43ed-9764-9d7e33548c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Generator, PatchDiscriminator\n",
    "\n",
    "# Setup for device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "G = Generator(3).to(device)\n",
    "D = PatchDiscriminator(3).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ab1b18-1ae1-410d-974b-97ea95885048",
   "metadata": {},
   "source": [
    "### Loss Function Setup\n",
    "\n",
    "The generator is trained using a combination of:\n",
    "- **Adversarial loss** (from the discriminator)\n",
    "- **Pixel-wise loss** (L1 or MSE)\n",
    "- **Perceptual loss** (optional, encourages semantic similarity)\n",
    "- **Structural Similarity Index Measure (SSIM)** loss (optional, to emphasize image structure)\n",
    "\n",
    "This combination helps the network generate visually convincing and anatomically accurate virtual stains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f7db68-a97c-4e52-923a-95ae470d820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from perceptual import VGGPerceptualLoss\n",
    "from pytorch_msssim import ssim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Model Training Parameters\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = nn.MSELoss()  # Adversarial loss (mean squared error)\n",
    "pixelwise_loss = nn.L1Loss()  # Pixel-wise loss (L1 loss)\n",
    "percep_loss_fn = VGGPerceptualLoss().to(device)\n",
    "ssim = StructuralSimilarityIndexMeasure(data_range=2.0).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8402ccc0-79c8-4f65-8e2a-2a81f70789db",
   "metadata": {},
   "source": [
    "### Loss Warmup and Scheduling Strategy\n",
    "\n",
    "To stabilize training and balance the influence of different objectives, we implement a dynamic scheduling strategy for loss components. This approach gradually adjusts the contribution of each loss during training:\n",
    "\n",
    "- **SSIM Loss Warmup**: We gradually increase the weight of SSIM loss from a small value to its full value over the course of training. This allows the model to learn coarse structural mappings before being penalized for fine structural differences.\n",
    "\n",
    "- **Perceptual Loss Warmup**: We warm up the perceptual loss (based on VGG features) so that it becomes more prominent as the model gains capacity to reproduce semantically meaningful structures:\n",
    "\n",
    "- **L1 Loss Decay**: To prevent over-reliance on pixel-wise accuracy and encourage perceptual realism, we gradually decay the weight of the L1 loss:\n",
    "\n",
    "This ensures that early training focuses on alignment and pixel fidelity, while later stages prioritize perceptual quality and realism. Together, these schedules help balance low-level fidelity and high-level realism during virtual stain generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aceaab-28fe-4b5c-aa86-6308cb190a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lambda_ssim(curr_step, start=10, end=15, total_steps=50000):\n",
    "    progress = min(curr_step / total_steps, 1.0)\n",
    "    return start + progress * (end - start)\n",
    "\n",
    "def get_lambda_percep(curr_step, start=0, end=0.5, total_steps=50000):\n",
    "    progress = min(curr_step / total_steps, 1.0)\n",
    "    return start + progress * (end - start)\n",
    "\n",
    "def decay_lambda_l1(curr_step, start=10, end=2, total_steps=50000):\n",
    "    progress = min(curr_step / total_steps, 1.0)\n",
    "    return start - progress * (start - end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f832fe2b-91da-443c-a48a-d8480f7f3484",
   "metadata": {},
   "source": [
    "### Training Parameter Setup\n",
    "\n",
    "Here we define the training parameters including learning rates, number of training steps, and loss lambda values.\n",
    "\n",
    "We also include linear decay schedules for lambda values and optimizer learning rates. These settings control how quickly the model learns and how the importance of each loss component evolves during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca1bd86-c23b-4e92-8fd1-8612b51ee31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rates\n",
    "lr_G = 0.0002 \n",
    "lr_D = 0.0001\n",
    "\n",
    "total_steps = 150000\n",
    "lambda_stop = int(0.3 * total_steps)\n",
    "start_decay = 75000\n",
    "\n",
    "# === Training Params ===\n",
    "log_interval = 100\n",
    "curr_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7fc25c-d3af-4012-9386-ba0114117e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LR Scheduler: Linear decay after 25k steps to 50k ===\n",
    "def linear_decay_lambda(step, start=start_decay, end=total_steps):\n",
    "    if step < start:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return max(0.0, 1.0 - (step - start) / (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6adaea2-a3f9-47f7-bb2b-ad95cb05687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=lr_G, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=lr_D, betas=(0.5, 0.999))\n",
    "scheduler_G = LambdaLR(optimizer_G, lr_lambda=linear_decay_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27cc6b8-3f43-4328-9195-1349002e8c4d",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "This is the main training loop where the generator and discriminator are updated iteratively.\n",
    "\n",
    "Each training step includes:\n",
    "- Forward passes through both networks\n",
    "- Computation of adversarial, pixel, perceptual, and SSIM losses\n",
    "- Warmup and decay of loss weights\n",
    "- Backpropagation and optimizer updates\n",
    "\n",
    "We periodically save checkpoints based on the validation SSIM and visualize model outputs to monitor training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbcc8aa-251c-4c83-bddb-9524f618614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "cyclic_loader = cycle(train_virtual_staining_dataloader)\n",
    "best_ssim = 0\n",
    "\n",
    "for real_images, target_images in cyclic_loader:\n",
    "    G.train()\n",
    "    \n",
    "    if curr_step >= total_steps:\n",
    "        break\n",
    "\n",
    "    # === Move to device ===\n",
    "    real_images = real_images.to(device)\n",
    "    target_images = target_images.to(device)\n",
    "\n",
    "    # === Discriminator ===\n",
    "    optimizer_D.zero_grad()\n",
    "    fake_images = G(real_images)\n",
    "\n",
    "    real_preds = D(target_images)\n",
    "    fake_preds = D(fake_images.detach())\n",
    "    real_loss = adversarial_loss(real_preds, torch.ones_like(real_preds))\n",
    "    fake_loss = adversarial_loss(fake_preds, torch.zeros_like(fake_preds))\n",
    "    d_loss = (real_loss + fake_loss) / 2\n",
    "    d_loss.backward()\n",
    "    optimizer_D.step()\n",
    "\n",
    "    # === Generator ===\n",
    "    optimizer_G.zero_grad()\n",
    "    fake_preds = D(fake_images)\n",
    "    g_adv = adversarial_loss(fake_preds, torch.ones_like(fake_preds))\n",
    "    g_l1 = pixelwise_loss(fake_images, target_images)\n",
    "    g_ssim = 1 - ssim(fake_images, target_images)\n",
    "    g_percep = percep_loss_fn(fake_images, target_images)\n",
    "    g_loss = g_adv + decay_lambda_l1(curr_step, total_steps=lambda_stop) * g_l1 + get_lambda_ssim(curr_step, total_steps=lambda_stop) * g_ssim + get_lambda_percep(curr_step, total_steps=lambda_stop) * g_percep\n",
    "    g_loss.backward()\n",
    "    optimizer_G.step()\n",
    "    scheduler_G.step()\n",
    "\n",
    "    # === Logging & Saving ===\n",
    "    if curr_step % log_interval == 0 or (curr_step + 1 >= total_steps):\n",
    "        lr = scheduler_G.get_last_lr()[0]\n",
    "        print(f\"[Step {curr_step}] D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}, LR: {lr:.6f}\")\n",
    "\n",
    "        # === Visualization ===\n",
    "        G.eval()\n",
    "        val_ssim = 0.0\n",
    "        with torch.no_grad():\n",
    "            for real_images, target_images in val_virtual_staining_dataloader:\n",
    "                out = G(real_images.to(device))\n",
    "                val_ssim += ssim(out, target_images.to(device))\n",
    "\n",
    "            val_ssim /= len(val_dataloader)\n",
    "            print(f\"[Step {curr_step}] Valid SSIM: {val_ssim.item():.4f}\")\n",
    "\n",
    "            real_images, target_images = next(iter(val_virtual_staining_dataloader))\n",
    "            \n",
    "            he_sample = real_images[:4]\n",
    "            ihc_sample = target_images[:4]\n",
    "            ihc_pred = G(he_sample)\n",
    "            def to_img(x): return (x * 0.5 + 0.5).clamp(0, 1)\n",
    "\n",
    "            fig, axs = plt.subplots(3, 4, figsize=(12, 9))\n",
    "            for j in range(4):\n",
    "                axs[0, j].imshow(to_img(he_sample[j].permute(1, 2, 0)).cpu())\n",
    "                axs[0, j].set_title(\"H&E\")\n",
    "                axs[1, j].imshow(to_img(ihc_sample[j].permute(1, 2, 0)).cpu())\n",
    "                axs[1, j].set_title(f\"Real IHC\")\n",
    "                axs[2, j].imshow(to_img(ihc_pred[j].permute(1, 2, 0)).cpu())\n",
    "                axs[2, j].set_title(f\"Virtual IHC\")\n",
    "                for row in axs: row[j].axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"/ix1/qgu/ngl18/GLBIO2025_VirtualStaining/samples/step_{curr_step}.png\")\n",
    "            plt.close()\n",
    "\n",
    "        # === Save Checkpoint ===\n",
    "        torch.save({\n",
    "            \"step\": curr_step,\n",
    "            \"G\": G.state_dict(),\n",
    "            \"D\": D.state_dict(),\n",
    "            \"optimizer_G\": optimizer_G.state_dict(),\n",
    "            \"optimizer_D\": optimizer_D.state_dict(),\n",
    "            \"scheduler_G\": scheduler_G.state_dict(),\n",
    "        }, f\"/ix1/qgu/ngl18/GLBIO2025_VirtualStaining/ckpts/last.pth\")\n",
    "\n",
    "        if val_ssim.item() >= best_ssim:\n",
    "            torch.save({\n",
    "            \"step\": curr_step,\n",
    "            \"val_ssim\": val_ssim.item(),\n",
    "            \"G\": G.state_dict(),\n",
    "            \"D\": D.state_dict(),\n",
    "            \"optimizer_G\": optimizer_G.state_dict(),\n",
    "            \"optimizer_D\": optimizer_D.state_dict(),\n",
    "            \"scheduler_G\": scheduler_G.state_dict(),\n",
    "        }, f\"/ix1/qgu/ngl18/GLBIO2025_VirtualStaining/ckpts/best.pth\")\n",
    "\n",
    "    curr_step += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528ea70e",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "To assess the quality of the virtually stained images, we use several complementary metrics:\n",
    "\n",
    "- **Structural Similarity Index Measure (SSIM)**: Quantifies perceptual similarity in terms of luminance, contrast, and structural information. Values range from 0 to 1, with 1 indicating perfect similarity.\n",
    "- **Peak Signal-to-Noise Ratio (PSNR)**: Evaluates the ratio between the maximum possible signal and the noise in the generated image. Higher PSNR indicates better image quality.\n",
    "- **Fr√©chet Inception Distance (FID)**: Compares the distribution of real and generated images in a deep feature space (typically using InceptionV3). Lower FID scores indicate greater similarity between real and synthetic image distributions.\n",
    "- \n",
    "These metrics together provide insight into both low-level pixel accuracy and high-level perceptual quality of the virtual stains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006e647e-27db-45cf-9dd9-b647a55f3530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pix2pix(generator, val_dataloader, device=\"cuda\"):\n",
    "    # Define metrics\n",
    "    fid = FrechetInceptionDistance(feature=2048, normalize=True)\n",
    "    psnr = PeakSignalNoiseRatio(data_range=1.0)  # Data range should be 1.0 for normalized images\n",
    "    ssim = StructuralSimilarityIndexMeasure(data_range=1.0)\n",
    "\n",
    "    # Move to device\n",
    "    fid = fid.to(device)\n",
    "    psnr = psnr.to(device)\n",
    "    ssim = ssim.to(device)\n",
    "    \n",
    "    # Ensure generator is in eval mode\n",
    "    generator.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for source, target in tqdm(val_dataloader):  # A = Input image, B = Ground truth\n",
    "            source, target = source.to(device), target.to(device)\n",
    "\n",
    "            # Generate fake images\n",
    "            fake_target = generator(source).to(device)\n",
    "\n",
    "            # Convert from [-1, 1] to [0, 1] for Inception (FID expects float in [0,255])\n",
    "            target = ((target + 1) / 2).clamp(0, 1)\n",
    "            fake_target = ((fake_target + 1) / 2).clamp(0, 1) \n",
    "\n",
    "            # Update FID metric\n",
    "            fid.update(target.float(), real=True)   \n",
    "            fid.update(fake_target.float(), real=False)  \n",
    "\n",
    "            # Update SSIM and PSNR metrics\n",
    "            ssim.update(fake_target, target)  # Fixed variable names\n",
    "            psnr.update(fake_target, target)  \n",
    "\n",
    "    # Compute final metrics\n",
    "    fid_score = fid.compute()\n",
    "    ssim_score = ssim.compute()\n",
    "    psnr_score = psnr.compute()\n",
    "    \n",
    "    print(f\"FID Score: {fid_score.item()}\")\n",
    "    print(f\"SSIM Score: {ssim_score.item()}\")\n",
    "    print(f\"PSNR Score: {psnr_score.item()}\")\n",
    "\n",
    "    return fid_score, ssim_score, psnr_score  # Return values for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9ead83-3edc-47f4-a95f-884c971c1fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Testing\n",
    "test_source_files = sorted(list(glob.iglob(f\"{root_dir}/data/test/he/**.npy\")))\n",
    "test_target_files = sorted(list(glob.iglob(f\"{root_dir}/data/test/ki67/**.npy\")))\n",
    "test_virtual_staining_dataset = VirtualStainingDataset(test_source_files, test_target_files, \n",
    "                                                       transform=val_test_transform)\n",
    "test_virtual_staining_dataloader = DataLoader(test_virtual_staining_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517f9fcc-ca61-49e8-bd9e-0624c2957b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Checkpoint\n",
    "ckpt = torch.load(f\"/ix1/qgu/ngl18/GLBIO2025_VirtualStaining/ckpts/best.pth\")\n",
    "G = G.load_state_dict(ckpt[\"G\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b17f05f-26d7-492c-9af6-4a10d4aa78c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_pix2_pix(G, test_virtual_staining_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57940bc-85f7-4875-bcb6-30421bf4abc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
