{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c07c874d-118d-4a5f-92b2-6d67a4b129e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6066e9-7ab5-46bb-bb8e-d4384867ccf2",
   "metadata": {},
   "source": [
    "## Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "163f94d0-1f86-4218-87f1-33de268fa60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/ix1/qgu/ngl18/VSCCset1VirtualStaining/\"\n",
    "source_files = sorted(list(glob.iglob(f\"{root_dir}/**/he/**.npy\")))\n",
    "target_files = sorted(list(glob.iglob(f\"{root_dir}/**/ihc/**.npy\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9d6a9c7-36b1-4c81-b93b-9f1f13d32b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import VirtualStainingDataset\n",
    "joint_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Affine(scale=(0.98, 1.02), translate_percent=0.05, rotate=(-5, 5), p=0.7),\n",
    "    A.ElasticTransform(alpha=1, sigma=50, alpha_affine=5, p=0.2),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05, p=0.7),\n",
    "    A.RandomGamma(p=0.3),\n",
    "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    ToTensorV2()\n",
    "], additional_targets={'target': 'image'}) \n",
    "\n",
    "virtual_staining_dataset = VirtualStainingDataset(source_files, target_files, transform=joint_transform)\n",
    "virtual_staining_dataloader = DataLoader(virtual_staining_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09115e66-1b69-41b2-bdc6-d74c3e80c6b4",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc821bb6-fe47-43ed-9764-9d7e33548c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Generator, PatchDiscriminator\n",
    "\n",
    "# Setup for device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "G = Generator(3).to(device)\n",
    "D = PatchDiscriminator(3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96f7db68-a97c-4e52-923a-95ae470d820f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ix1/qgu/ngl18/envs/vscc/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/ix1/qgu/ngl18/envs/vscc/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from perceptual import VGGPerceptualLoss\n",
    "from pytorch_msssim import ssim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Model Training Parameters\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = nn.MSELoss()  # Adversarial loss (mean squared error)\n",
    "pixelwise_loss = nn.L1Loss()  # Pixel-wise loss (L1 loss)\n",
    "percep_loss_fn = VGGPerceptualLoss().to(device)\n",
    "\n",
    "# Learning Rates\n",
    "lr_G = 0.0002 \n",
    "lr_D = 0.0001\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=lr_G, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=lr_D, betas=(0.5, 0.999))\n",
    "\n",
    "total_steps = 150000\n",
    "lambda_stop = int(0.3 * total_steps)\n",
    "start_decay = 75000\n",
    "\n",
    "\n",
    "# === Scheduler: Linear decay after 25k steps to 50k ===\n",
    "def linear_decay_lambda(step, start=start_decay, end=total_steps):\n",
    "    if step < start:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return max(0.0, 1.0 - (step - start) / (end - start))\n",
    "\n",
    "# === Training Params ===\n",
    "log_interval = 5000\n",
    "curr_step = 0\n",
    "\n",
    "scheduler_G = LambdaLR(optimizer_G, lr_lambda=linear_decay_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fe7d069-d0d1-46a2-87df-422ceeb56265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lambda_ssim(curr_step, start=1, end=5.0, total_steps=50000):\n",
    "    progress = min(curr_step / total_steps, 1.0)\n",
    "    return start + progress * (end - start)\n",
    "\n",
    "def get_lambda_percep(curr_step, start=1, end=3.0, total_steps=50000):\n",
    "    progress = min(curr_step / total_steps, 1.0)\n",
    "    return start + progress * (end - start)\n",
    "\n",
    "def decay_lambda_l1(curr_step, start=10, end=2, total_steps=50000):\n",
    "    progress = min(curr_step / total_steps, 1.0)\n",
    "    return start - progress * (start - end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27cc6b8-3f43-4328-9195-1349002e8c4d",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbcc8aa-251c-4c83-bddb-9524f618614b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 0] D Loss: 0.4288, G Loss: 9.0413, LR: 0.000200\n"
     ]
    }
   ],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "cyclic_loader = cycle(virtual_staining_dataloader)\n",
    "\n",
    "for real_images, target_images in cyclic_loader:\n",
    "    if curr_step >= total_steps:\n",
    "        break\n",
    "\n",
    "    # === Move to device ===\n",
    "    real_images = real_images.to(device)\n",
    "    target_images = target_images.to(device)\n",
    "\n",
    "    # === Discriminator ===\n",
    "    optimizer_D.zero_grad()\n",
    "    fake_images = G(real_images)\n",
    "\n",
    "    real_preds = D(target_images)\n",
    "    fake_preds = D(fake_images.detach())\n",
    "    real_loss = adversarial_loss(real_preds, torch.ones_like(real_preds))\n",
    "    fake_loss = adversarial_loss(fake_preds, torch.zeros_like(fake_preds))\n",
    "    d_loss = (real_loss + fake_loss) / 2\n",
    "    d_loss.backward()\n",
    "    optimizer_D.step()\n",
    "\n",
    "    # === Generator ===\n",
    "    optimizer_G.zero_grad()\n",
    "    fake_preds = D(fake_images)\n",
    "    g_adv = adversarial_loss(fake_preds, torch.ones_like(fake_preds))\n",
    "    g_l1 = pixelwise_loss(fake_images, target_images)\n",
    "    g_ssim = 1 - ssim(fake_images, target_images, data_range=2.0, size_average=True)\n",
    "    g_percep = percep_loss_fn(fake_images, target_images)\n",
    "    g_loss = g_adv + decay_lambda_l1(curr_step, total_steps=lambda_stop) * g_l1 + get_lambda_ssim(curr_step, total_steps=lambda_stop) * g_ssim + get_lambda_percep(curr_step, total_steps=lambda_stop) * g_percep\n",
    "    g_loss.backward()\n",
    "    optimizer_G.step()\n",
    "    scheduler_G.step()\n",
    "\n",
    "    # === Logging & Saving ===\n",
    "    if curr_step % log_interval == 0 or (curr_step + 1 >= total_steps):\n",
    "        lr = scheduler_G.get_last_lr()[0]\n",
    "        print(f\"[Step {curr_step}] D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}, LR: {lr:.6f}\")\n",
    "\n",
    "        # === Visualization ===\n",
    "        D.eval()\n",
    "        with torch.no_grad():\n",
    "            he_sample = real_images[:4]\n",
    "            ihc_sample = target_images[:4]\n",
    "            ihc_pred = G(he_sample)\n",
    "            def to_img(x): return (x * 0.5 + 0.5).clamp(0, 1)\n",
    "\n",
    "            fig, axs = plt.subplots(3, 4, figsize=(12, 9))\n",
    "            for j in range(4):\n",
    "                axs[0, j].imshow(to_img(he_sample[j].permute(1, 2, 0)).cpu())\n",
    "                axs[0, j].set_title(\"H&E\")\n",
    "                axs[1, j].imshow(to_img(ihc_sample[j].permute(1, 2, 0)).cpu())\n",
    "                axs[1, j].set_title(f\"Real IHC\")\n",
    "                axs[2, j].imshow(to_img(ihc_pred[j].permute(1, 2, 0)).cpu())\n",
    "                axs[2, j].set_title(f\"Virtual IHC\")\n",
    "                for row in axs: row[j].axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"/ix1/qgu/ngl18/VirtualStaining/outputs/vscc/step_{curr_step}.png\")\n",
    "            plt.close()\n",
    "\n",
    "        # === Save Checkpoint ===\n",
    "        torch.save({\n",
    "            \"step\": curr_step,\n",
    "            \"generator\": G.state_dict(),\n",
    "            \"discriminator\": D.state_dict(),\n",
    "            \"optimizer_G\": optimizer_G.state_dict(),\n",
    "            \"optimizer_D\": optimizer_D.state_dict(),\n",
    "            \"scheduler_G\": scheduler_G.state_dict(),\n",
    "        }, f\"/ix1/qgu/ngl18/VirtualStaining/ckpts/vscc/step_{curr_step}.pth\")\n",
    "\n",
    "    curr_step += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73df6499-4c46-46bb-a365-71a5d2496a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
